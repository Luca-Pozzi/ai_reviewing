{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI reviewing\n",
    "An AI-powered tool to support the literature review process.\n",
    "This notebook implements *Retrieval-Augmented Generation* (RAG), i.e. a technique that grants a generative model to fetch information from (in this case) files in the local system.\n",
    "Leveraging on the power of RAG, the notebook allows the user to query the paper and gather insights without having to read the full-text.\n",
    "\n",
    "Two different pipelines are shown in this notebook:\n",
    "* [Step-by-step execution](#step-by-step-execution)<br>Run each step of the processing and have a look at intermediate output of the processing to gain an understanding of what is going on under the hood.\n",
    "\n",
    "* [Batch execution](#batch-execution)<br>Run the pipeline on a set of references. Meant to be used in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by clearing the workspace. Previous execution of the present notebook\n",
    "# may have initialized models that occupy a large space in the computer memory.\n",
    "# Execute the cell for a fresh start.\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the paths to the models used in the application, namely:\n",
    "* the path to the model to generate embeddings\n",
    "* the path to the large language model that will answer to queries\n",
    "\n",
    "You can manually define the paths, or set them to `None`. In the latter case, a filedialog will pop-up to let you pick the file by browsing the filesystem.\n",
    "\n",
    "To install `gpt4all` and get the needed models, you can follow the official [GPT4All Python SDK documentation](https://docs.gpt4all.io/gpt4all_python/home.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tkinter as tk # for filedialogs\n",
    "from tkinter import filedialog\n",
    "\n",
    "# Start by defining paths to relevant components of the application, namely:\n",
    "#   * the path to the model to generate embeddings\n",
    "EMBED_MODEL = \"C:\\\\Users\\\\pozzi\\\\gpt4all\\\\resources\\\\nomic-embed-text-v1.5.f16.gguf\"\n",
    "if EMBED_MODEL is None:\n",
    "    file = filedialog.askopenfile()\n",
    "    if file:\n",
    "        EMBED_MODEL = os.path.abspath(file.name)\n",
    "\n",
    "#   * the path to the large language model\n",
    "LLM_MODEL = \"C:\\\\Users\\\\pozzi\\\\AppData\\\\Local\\\\nomic.ai\\\\GPT4All\\\\Meta-Llama-3-8B-Instruct.Q4_0.gguf\"\n",
    "if LLM_MODEL is None:\n",
    "    file = filedialog.askopenfile()\n",
    "    if file:\n",
    "        LLM_MODEL = os.path.abspath(file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-step execution\n",
    "This is meant as an instructive example to better understand how the pipeline works. Each step is manually triggered by the user.\n",
    "\n",
    "If you already know how RAG works, and/or have previous experience with this tools, you can collapse this section and move to the [Batch Execution section](ai_reviewing.ipynb#batch-execution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can manually define the path to the PDF file to inspect, or set it to None. In the latter case, a filedialog will pop-up to let you pick the file by browsing the filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PDF file to process\n",
    "# Must be:\n",
    "#    * valid filepath to PDF file\n",
    "#    * `None` (opens a filedialog to select the file)\n",
    "FILE_PATH = None \n",
    "#FILE_PATH = \"C:\\\\Users\\\\pozzi\\\\OneDrive - Politecnico di Milano\\\\review_article\\\\code\\\\refs\\\\_to_review_short\\\\files\\\\1875\\\\Maroto-GÃ³mez et al. - 2023 - Bio-inspired Cognitive Decision-making to Personal.pdf\" \n",
    "\n",
    "if FILE_PATH is None:\n",
    "    file = filedialog.askopenfile()\n",
    "    if file:\n",
    "        FILE_PATH = os.path.abspath(file.name)\n",
    "\n",
    "# Check for type extension\n",
    "if not os.path.splitext(FILE_PATH)[-1] == '.pdf':\n",
    "    raise ValueError(\"Unsupported file extension {}. Only PDF files are supported\".format(os.path.splitext(FILE_PATH)[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loading and splitting\n",
    "The PDF document is loaded and parsed to extract the actual content. The content is then split into smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the PDF file and read its content\n",
    "loader = PyMuPDFLoader(FILE_PATH)\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the content into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512,\n",
    "                                               chunk_overlap=128, \n",
    "                                               add_start_index=True\n",
    "                                              )\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "if not chunks:\n",
    "    raise ValueError('Document at {} produced no chunks'.format(FILE_PATH))\n",
    "else:\n",
    "    print(\"A sample from the middle of your file:\\n\\n\" +\n",
    "          \"{}\".format(chunks[len(chunks)//2].page_content)\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings\n",
    "In order to later feed it into the LLM model, we want to store the *meaning* of a text document. But computers do like numbers rather than raw text. \n",
    "\n",
    "Document embeddings are a way of converting a document's text into high-dimensional vectors that captures their meaning. The result is a numerical representation of the text, where similar contents (i.e. `chunks`) have similar representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Initialize the embedding model\n",
    "if os.path.isfile(EMBED_MODEL):\n",
    "    allow_download=False\n",
    "else:\n",
    "    print('{} is not a valid file in the local system.\\nGPT4All will try to download the model from gpt4all.io'.format(EMBED_MODEL))\n",
    "    allow_download=True\n",
    "model_kwargs = {'allow_download':allow_download}    \n",
    "embed_model = GPT4AllEmbeddings(model_name=EMBED_MODEL,\n",
    "                                device=\"cpu\",\n",
    "                                gpt4all_kwargs=model_kwargs\n",
    "                                )\n",
    "\n",
    "# Initialize the vector store (i.e. database) to store the embeddings\n",
    "db = FAISS.from_documents(documents=chunks, \n",
    "                          embedding=embed_model\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated embeddings can now be exploited to evaluate similarity between contents by evaluating a numerical distance.<br>The cell below just print the first chunk returned by the retriever. You may observe that it is not necessarily the best one to answer the query. For this reason, it is advisable to set the `k` parameter to a number > 1. Keep in mind that increasing `k` will also increase the computational effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve chunk(s) of document(s) that are relevant to the query\n",
    "query = 'What is the robot name?'\n",
    "retriever = db.as_retriever(search_type=\"mmr\", #\"similarity\",\n",
    "                            search_kwargs={\"k\":6,\n",
    "                                            \"fetch_k\":20,\n",
    "                                            \"lambda_mult\":0.5,\n",
    "                                            #\"filter\":{\"source\":FILE_PATH}\n",
    "                                        })\n",
    "print(retriever.invoke(query)[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation\n",
    "Exploit the above steps to give context to feed in context to a Large Language Model, giving it access to the contents of your interest.\n",
    "\n",
    "Here a [chain](https://python.langchain.com/docs/how_to/sequence/) including the retriever, the prompt, the LLM, and the output parser is defined. The output of each component is fed as input to the following one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"You are an helpful research assistant. Please give answer only based on the following context: {context}. If you don't know something, just say you don't know.\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "local_path = (\"C:\\\\Users\\\\pozzi\\\\AppData\\\\Local\\\\nomic.ai\\\\GPT4All\\\\Meta-Llama-3-8B-Instruct.Q4_0.gguf\")\n",
    "if 'llm' in locals():\n",
    "    # NOTE. Attempting to create a new instance of the model (e.g. if you want\n",
    "    # to play with the params) might fail due to lack of memory. Before\n",
    "    # instantiating, verify that no other model exist.\n",
    "    del llm\n",
    "llm = GPT4All(model=local_path,\n",
    "              device=\"gpu\",\n",
    "              max_tokens=2048,\n",
    "              n_predict=4096,\n",
    "              temp=0.7,\n",
    "              top_p=0.4,\n",
    "              top_k=40,\n",
    "              repeat_last_n=64,\n",
    "              repeat_penalty=1.18\n",
    "              )\n",
    "\n",
    "# Put everything together\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "if 'rag_chain' in locals():\n",
    "    # NOTE. Attempting to create a new instance of the chain might fail due to \n",
    "    # lack of memory. Before instantiating, verify that no other chain exist.\n",
    "    del rag_chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your query and give it to the RAG to get the answer based on the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"What is the robot name?\"\"\"\n",
    "\n",
    "print(query)\n",
    "output = rag_chain.invoke(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch execution\n",
    "After having understood how the pipeline works, you can leverage on RAG to help with literature reviewing.\n",
    "> **NOTE.** It is advisable to start from scratch, clearing all the variables. To do so, run the first two cells and just skip the **Step-by-step execution** section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the path to the directory where the BIB file and articles PDF are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   * the path to the folder where the Zotero references has been exported\n",
    "REFS_PATH = \"C:\\\\Users\\\\pozzi\\\\OneDrive - Politecnico di Milano\\\\review_article\\\\code\\\\refs\\\\_full_text\"\n",
    "if REFS_PATH is None:\n",
    "    dir = filedialog.askdirectory(initialdir='./refs')\n",
    "    if dir:\n",
    "        REFS_PATH = os.path.abspath(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize useful objects\n",
    "Create the instance of the LLM that will be used for the RAG application, and define the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Get ready to split the content into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512,\n",
    "                                               chunk_overlap=128, \n",
    "                                               add_start_index=True\n",
    "                                              )\n",
    "\n",
    "# Initialize the embedding model\n",
    "if os.path.isfile(EMBED_MODEL):\n",
    "    allow_download=False\n",
    "else:\n",
    "    print('{} is not a valid file in the local system.\\nGPT4All will try to download the model from gpt4all.io'.format(EMBED_MODEL))\n",
    "    allow_download=True\n",
    "model_kwargs = {'allow_download':allow_download}    \n",
    "embed_model = GPT4AllEmbeddings(model_name=EMBED_MODEL,\n",
    "                                device=\"cpu\",\n",
    "                                gpt4all_kwargs=model_kwargs\n",
    "                                )\n",
    "\n",
    "# Initialize the LLM model\n",
    "template = \"\"\"I am an engineer using large language model for scientific paper summarization. Given the context:\\n\\n{context}\\n\\nplease answer to the question below. If you don't know something, just say you don't know.\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "if 'llm' in locals():\n",
    "    # NOTE. Attempting to create a new instance of the model (e.g. if you want\n",
    "    # to play with the params) might fail due to lack of memory. Before\n",
    "    # instantiating, verify that no other model exist.\n",
    "    del llm\n",
    "llm = GPT4All(model=LLM_MODEL,\n",
    "              device=\"gpu\",\n",
    "              max_tokens=4096,\n",
    "              n_predict=4096,\n",
    "              temp=0.7,\n",
    "              top_p=0.4,\n",
    "              top_k=40,\n",
    "              repeat_last_n=64,\n",
    "              repeat_penalty=1.18\n",
    "             )\n",
    "\n",
    "# Specify what do you want the model to do\n",
    "query = \"\"\"I would like you to generate a digest of the paper {}, highlighting:\n",
    "    * **Robot.** Characteristics of the device used in the study.\n",
    "    * **Application.** What is the robot used for.\n",
    "    * **Population.** Demographics (total number, age, health status) of subjects taking part to the experimental campaign.\n",
    "    * **Duration.** Number of interaction of each subject with the robot. Duration of each interaction.\n",
    "    * **Setting.** Where is the experimental campaign conducted (nursing home, hospital, retirement village, private homesâ¦).\n",
    "    * **Interaction.** How the user can interact with the robot -and vice versa- during the task (e.g. vocal, touch screen, controller, gesturesâ¦).\n",
    "    * **Outcome measures.** How is the application evaluated in terms of: (i) technical accuracy, (ii) user experience.\n",
    "    * **Results.** Quantitative evaluation of the outcome measures.\n",
    "\n",
    "    Please return max 500 words per each bullet point.\n",
    "    \"\"\"\n",
    "\n",
    "n_features = query.count('* **')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query definition\n",
    "Define the query to be passed to the RAG.\n",
    "\n",
    "The query should be formatted as a dictionary (see example below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'intro': 'I would like you to generate a digest of the paper, highlighting the following features:',\n",
    "         'features': [\n",
    "             # (<feature name>, <question string>, <Excel file column>)\n",
    "             ('Robot', 'Which are the characteristics of the robot used in the study?', 'F'),\n",
    "             ('Application', 'What is the robot used for?', 'G'),\n",
    "             ('Population', 'How many people participated to the testing? What is their age? Do they suffer from special health conditions?', 'H'),\n",
    "             ('Duration', 'How many times does each user interact with the robot? How long does each interaction take?', 'I'),\n",
    "             ('Setting', 'Where is the experimental campaign conducted?', 'J'),\n",
    "             ('Interaction', 'How can the user interact with the robot -and vice versa- during the task? E.g. vocal, touch screen, controller, gestures...', 'K'),\n",
    "             ('Outcome measures', 'Which metrics are used to evaluate the application in terms of technical accuracy? Which metrics are used to evaluate the application in terms of user experience?', 'L'),\n",
    "             ('Results', 'What are the quantitative results? What are the qualitative results of the study?', 'M')\n",
    "            ],\n",
    "    'constraints': 'Please answer in max 500 words',\n",
    "    'outro': 'per each bullet point'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a reference list from an exported Zotero library \n",
    "See [README.md](README.md) for instructions on how to create and export your library.\n",
    "\n",
    "> **NOTE.** The cell below has to be executed only once per collection. Once the Excel file has been created, you should skip the execution of the cell and move directly to [Generate digest](#generate-digest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from pybtex.database.input import bibtex\n",
    "import openpyxl\n",
    "\n",
    "def row_col_to_cell(row, col):\n",
    "    if col < 1 or col > 26:\n",
    "        raise ValueError('The specified `col` value {} cannot be converted to a capital letter. Indeed, it would return {}'.format(col, chr(col+64)))\n",
    "    return chr(col + 64) + str(row)\n",
    "\n",
    "# Read the BibTeX file\n",
    "# NOTE. The BibTex file MUST be named as the folder that contains it.\n",
    "refs_folder_name = os.path.basename(os.path.normpath(REFS_PATH))\n",
    "bib_parser = bibtex.Parser()\n",
    "bib_data = bib_parser.parse_file(os.path.join(REFS_PATH,\n",
    "                                              refs_folder_name + '.bib'\n",
    "                                              ))\n",
    "# Write the refs to an Excel file\n",
    "wb = openpyxl.Workbook()\n",
    "ws = wb.active # get the active sheet in the workbook\n",
    "ws.title = 'References'\n",
    "# Create the header of the Excel table\n",
    "header = ['Key', 'Author', 'Title', 'Year', 'PDF']\n",
    "for col in range(1, 5+1): # cells in Excel are 1-based\n",
    "    ws[row_col_to_cell(row=1, col=col)] = header[col-1]\n",
    "# Populate the table\n",
    "row = 2\n",
    "for key in bib_data.entries.keys():\n",
    "    # Write the key value\n",
    "    ws[row_col_to_cell(row=row, col=1)] = key\n",
    "    entry = bib_data.entries[key]\n",
    "    # Get and write the authors names to one cell\n",
    "    authors_string = ''\n",
    "    for author in entry.persons['author']:\n",
    "        if not authors_string: # i.e. for the first author\n",
    "            authors_string += str(author)\n",
    "        else:\n",
    "            authors_string += ' and ' + str(author)\n",
    "    ws[row_col_to_cell(row=row, col=2)] = authors_string\n",
    "    # Get and the write the paper title and year of publication\n",
    "    ws[row_col_to_cell(row=row, col=3)] = entry.fields['title']\n",
    "    ws[row_col_to_cell(row=row, col=4)] = entry.fields['year']\n",
    "    # Get the file location within `REFS_PATH\\files`\n",
    "    pdf_rel_path =re.search(\"(?<=:files/)(.*)(?=:application)\", \n",
    "                            entry.fields['file']\n",
    "                            )\n",
    "    if pdf_rel_path: # i.e. if there is a match from regex\n",
    "        pdf_abs_path = os.path.normpath(os.path.join(REFS_PATH,\n",
    "                                                    'files',\n",
    "                                                    pdf_rel_path.group()\n",
    "                                                    ))\n",
    "        ws[row_col_to_cell(row=row, col=5)] = pdf_abs_path\n",
    "    row += 1 # move to the row belows\n",
    "\n",
    "# Save the produced Excel file\n",
    "if os.path.isfile(os.path.join(REFS_PATH, refs_folder_name + '.xlsx')):\n",
    "    # If the file already exists, append a string to the filename to avoid overwriting a useful file. \n",
    "    print('WARNING. The file {} already exists.'.format(refs_folder_name + '.xlsx'))\n",
    "    refs_folder_name += '_new'\n",
    "    print('The new file has been saved as {} to avoid overwriting.\\nManually rename the files before proceeding.'.format(refs_folder_name + '.xlsx'))\n",
    "wb.save(os.path.join(REFS_PATH,\n",
    "                     refs_folder_name + '.xlsx'\n",
    "                    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate digest\n",
    "Apply the query to references into the selected Excel file.\n",
    "\n",
    "Use the cell below to set the row indexes (1-based) of the Excel file to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_ROW = 0     # enter the desired value\n",
    "MAX_ROW = 10    # enter the desired value\n",
    "\n",
    "SINGLE_QUERY = False    \n",
    "# If True, all the `features` in the `query` are merged into a single string, organized in bullet points. This provides faster, but less accurate responses.\n",
    "# If False, each feature is passed as a single query. This involves N (where N is equal to `len(query['features'])`) calls to the RAG chain per each paper. The response time should be roughly N times slower, but the provided responses should be more accurate.\n",
    "if SINGLE_QUERY:\n",
    "    query_string = query['intro'] + '\\n'\n",
    "    for feature, question, __ in query['features']:\n",
    "        query_string += '* **{}.** {}\\n'.format(feature, question)\n",
    "    query_string += query['constraints'] + ' ' + query['outro']\n",
    "    query_list = [query_string]\n",
    "else:\n",
    "    query_list = [question for __, question, __ in query['features']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import openpyxl\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Load the Excel file\n",
    "# NOTE. The Excel file MUST be named as the folder that contains it.\n",
    "refs_folder_name = os.path.basename(os.path.normpath(REFS_PATH))\n",
    "refs_filepath = os.path.join(REFS_PATH,\n",
    "                             refs_folder_name + '.xlsx'\n",
    "                            )\n",
    "wb = openpyxl.load_workbook(filename=refs_filepath)\n",
    "ws = wb.active # get the active sheet in the workbook\n",
    "\n",
    "MIN_ROW = max(2, MIN_ROW)\n",
    "MAX_ROW = min(ws.max_row, MAX_ROW)\n",
    "for row_cnt, row in enumerate(ws.iter_rows(min_row=MIN_ROW, max_row=MAX_ROW)):\n",
    "    print(\"Processing ref {}/{}\".format(row_cnt+1, MAX_ROW-MIN_ROW+1))\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Load the PDF file and read its content\n",
    "    loader = PyMuPDFLoader(ws['E' + str(row_cnt + MIN_ROW)].value)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Split the content into chunks\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    if not chunks:\n",
    "        continue    # the PDF is likely generated as an image and it is thus\n",
    "                    # impossible to extract content from it.\n",
    "\n",
    "    # Create the vector store (i.e. database) to store the embeddings\n",
    "    db = FAISS.from_documents(documents=chunks, \n",
    "                            embedding=embed_model\n",
    "                            )\n",
    "    # Get ready to retrieve relevant sources from the vector store\n",
    "    retriever = db.as_retriever(search_type=\"mmr\", #\"similarity\",\n",
    "                                search_kwargs={\"k\":6,\n",
    "                                                \"fetch_k\":20,\n",
    "                                                \"lambda_mult\":0.5,\n",
    "                                                #\"filter\":{\"source\":FILE_PATH}\n",
    "                                            })\n",
    "    \n",
    "    # Create and call the RAG chain\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    first_column = ord('F')\n",
    "    n_features = len(query['features'])\n",
    "    for i, q in enumerate(query_list):\n",
    "        # Call the RAG chain\n",
    "        output = rag_chain.invoke(q)\n",
    "        print(output)\n",
    "        # Save one features per Excel column\n",
    "        row_idx = row_cnt + MIN_ROW\n",
    "        if SINGLE_QUERY:\n",
    "            # Parse the output to fill in the Excel file\n",
    "            # Get the file location within `REFS_PATH\\files`\n",
    "            fields = output.split('**')\n",
    "            try:\n",
    "                for i in range(n_features):\n",
    "                    ws[chr(first_column + i) + str(row_idx)] = fields [(i+1)*2]\n",
    "            except: \n",
    "                # If an error occurs while parsing, save the row output for later manual parsing\n",
    "                ws[chr(first_column + n_features) + str(row_idx)] = output\n",
    "        else:\n",
    "            ws[chr(first_column + i) + str(row_idx)] = output\n",
    "    del rag_chain\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if row_cnt%10 == 0 and not row_cnt == 0:\n",
    "        # Save the output every 10 items processed\n",
    "        wb.save(refs_filepath)\n",
    "\n",
    "wb.save(refs_filepath)\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
